{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Machine Learning Pipeline using SageMaker, EMR, S3, ECS, API Gateway and Lambda\n",
    "\n",
    "\n",
    "\n",
    "## Module-3: Preprocess Data, Build and Train a Spark Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin by running the following cell to confirm that this notebook is connected to EMR:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Define Project Name and S3 Bucket\n",
    "\n",
    "You're going to need two pieces of information going forward.  First, you need a name for the model you're working with -- you can name it anything you prefer, within the naming limitations.  You also need to pick an S3 bucket that you'll be uploading your model artifacts to.\n",
    "\n",
    "#### Substep a. Select a Model name, S3 bucket name, and an AWS region\n",
    "You can decide what to name the model and the S3 bucket. Pick unique names for each. This example will be using 'sm-emr-e2e-model' for model, but you will need to specify your own bucket, because they are globally unique. You will also pick an AWS region. The S3 Bucket will be created in your chosen AWS region in Step-4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "model_name = 'sm-emr-e2e-model' # change the model name to something unique\n",
    "s3_bucket = '<user defined S3 bucket>' # change the s3 bucket to something unique\n",
    "aws_region = 'us-east-2' # at the time of creation this tutorial will run in US-EAST-1 region"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Substep b. Define the model name and S3 bucket on EMR\n",
    "\n",
    "This notebook is running both on EMR and on a notebook instance, hence, you need to define the same model name and same S3 bucket on EMR as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EMR Cell\n",
    "model_name = 'sm-emr-e2e-model' # same as above\n",
    "s3_bucket = '<user defined S3 bucket>' # same as above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Setup Environment variables for Model Name and S3 Bucket on Sagemaker Notebook Instance\n",
    "\n",
    "This provides a portable way of using operating system dependent functionalities. This will enable you to call the model and/or the S3 bucket using bash command in subsequent steps. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "import os\n",
    "\n",
    "os.environ['SAGEMAKER_MODEL_NAME'] = model_name\n",
    "os.environ['SAGEMAKER_S3_BUCKET'] = s3_bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Create the S3 bucket from the Notebook instance\n",
    "\n",
    "Python SDK ‘boto3’ helps to connect to S3 from the Sagemaker notebook instance. The below code will let you create the S3 bucket defined in Step-2 to store the model and the artifacts. The bucket will be a private bucket and can be found in your AWS S3 account. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "import boto3\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "s3.create_bucket(Bucket=s3_bucket, CreateBucketConfiguration={'LocationConstraint': aws_region}) # This will create the bucket in US-east-1 region"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verfiy your work:\n",
    "You can verify, if the S3 bucket is created successfully in your S3 bucket, by running the below code block. If your bucket is created, it will be printed. Otherwise, please go back to the previous steps, and make sure you've the consistent s3 bucket names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "import boto3\n",
    "\n",
    "for bucket in s3.buckets.all():\n",
    "    if bucket.name == s3_bucket:\n",
    "        print(bucket.name + \" is successfully created in your S3 bucket!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Load data into S3 from the notebook instance\n",
    "\n",
    "You will be using Enron Email data set for this example. The data set consists of 1,227,255 emails with 493, 384 attachments covering 151 custodians. This data set is already provided to you in “data” folder of the GIT repository that you cloned earlier. \n",
    "\n",
    "Let's upload the data to an S3 Bucket.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "import sagemaker as sage\n",
    "\n",
    "sess = sage.Session()\n",
    "\n",
    "os.chdir(\"/home/ec2-user/SageMaker/sm-emr-e2e\")\n",
    "\n",
    "prefix = model_name\n",
    "WORK_DIRECTORY = \"data\"\n",
    "data_location = sess.upload_data(WORK_DIRECTORY, key_prefix=prefix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verify your work:\n",
    "\n",
    "Run the following code to list the uploaded data files from your S3 bucket. The data files must be in *.parquet* format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "import boto3\n",
    "\n",
    "for bucket in s3.buckets.all():\n",
    "    for obj in bucket.objects.filter(Prefix= model_name):\n",
    "        print('{0}:{1}'.format(bucket.name, obj.key))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Read the data in EMR Spark Cluster\n",
    "\n",
    "Now that the data is uploaded to S3, you can copy the S3 bucket path so that you could directly read the data from the EMR cluster.\n",
    "\n",
    "#### Substep a. Copy the S3 bucket file path\n",
    "The S3 bucket file path is required to read the data on EMR Spark. Copy and paste the below string into the Substep b.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that your data is uploaded to S3, let's find out the S3 location string to use it on EMR Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "print(\"Copy the following S3 location to the EMR cell below: \"+\"'\"+data_location+'/enron.parquet'+\"'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Substep b. Read the data in EMR spark Cluster\n",
    "Copy paste the above S3 bucket file path to read the data on EMR spark.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EMR cell\n",
    "df = spark.read.parquet(\"<copy and paste the S3 location here>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verify your work:\n",
    "Lets take a look at the dataframe to make sure everything loaded okay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EMR cell\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Split the data into bag of words\n",
    "\n",
    "You will split the email dataset to bag of words in order to prepare the data for model training.\n",
    "\n",
    "This example is using only 10% of the total data as a sample to run on a small EMR Cluster. If you want to do this exercise on the whole data, consider scaling up your EMR cluster to a large size and then changing the fraction value in the code block to *1.0*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EMR Cell\n",
    "from pyspark.sql.functions import split, col\n",
    "sample = df.sample(withReplacement=False, fraction=0.1).withColumn('bow', split(col('content'), ' '))\n",
    "sample.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Build a Spark pipeline and fit it to the sample data set on EMR\n",
    "\n",
    "Spark pipeline takes a vectorizer as input. Hence, you need to convert the bag of words to a vector. Once the model is fit, you need to hash it, convert into a parquet file and save it in S3.\n",
    "\n",
    "#### Substep a: Build and fit a spark pipeline\n",
    "You can build and train the model using the following spark pipeline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a basic Spark pipeline, and fit it to our dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EMR Cell\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import Word2Vec\n",
    "\n",
    "word2vec = Word2Vec(inputCol='bow', outputCol='features', vectorSize=10)\n",
    "\n",
    "pipeline = Pipeline(stages=[word2vec])\n",
    "\n",
    "# Fit the pipeline to training.\n",
    "model = pipeline.fit(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Substep b: Convert it to a parquet file\n",
    "You can create the hash file from the model and save it in S3.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EMR Cell\n",
    "hashes = model.transform(sample)\n",
    "hashes.write.parquet(\"s3://\"+s3_bucket+\"/models/hashes.parquet\", mode='overwrite')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verify your work:\n",
    "\n",
    "Run the following the code block to list the .parquet files that are saved in your S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "import boto3\n",
    "\n",
    "for bucket in s3.buckets.all():\n",
    "    if bucket.name == s3_bucket:\n",
    "        for obj in bucket.objects.filter(Delimiter='/models/hashes.parquet/'):\n",
    "            print('{0}:{1}'.format(bucket.name,obj.key))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Save the training model to S3 on EMR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, the fit function above was actually running on EMR in Spark, and not on the SageMaker notebook instance.  Once EMR has finished training, run the cell below to save your trained model to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EMR Cell\n",
    "model.write().overwrite().save(\"s3://\" + s3_bucket + \"/models/\" + model_name +  \".model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verfiy your work:\n",
    "\n",
    "Let's check if the training model is saved to s3 successfully. Run the following code cell to list the trained model (must be in .model format) from your S3 bucket along with *.parquet* files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "import boto3\n",
    "\n",
    "for bucket in s3.buckets.all():\n",
    "    if bucket.name == s3_bucket:\n",
    "        for obj in bucket.objects.filter(Delimiter='/models/'):\n",
    "            print('{0}:{1}'.format(bucket.name,obj.key))\n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9: Save model artifacts in a tar.gz format using Bash commands\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Substep a. Convert the model artifacts to tar.gz format\n",
    "\n",
    "SageMaker requires model artifacts to be in a tar.gz format.  Run the cell below to copy the model down from S3, archive and compress it, before sending it back our S3 working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "mkdir -p artifacts/$SAGEMAKER_MODEL_NAME.model\n",
    "mkdir -p artifacts/hashes\n",
    "\n",
    "cd artifacts/hashes\n",
    "aws s3 cp --recursive s3://$SAGEMAKER_S3_BUCKET/models/hashes.parquet ./\n",
    "cd ../..\n",
    "\n",
    "cd artifacts/$SAGEMAKER_MODEL_NAME.model\n",
    "aws s3 cp --recursive s3://$SAGEMAKER_S3_BUCKET/models/$SAGEMAKER_MODEL_NAME.model ./\n",
    "\n",
    "cd ../..\n",
    "tar -cvvf $SAGEMAKER_MODEL_NAME.model.tar ./artifacts\n",
    "gzip -f $SAGEMAKER_MODEL_NAME.model.tar\n",
    "aws s3 cp $SAGEMAKER_MODEL_NAME.model.tar.gz s3://$SAGEMAKER_S3_BUCKET/models/$SAGEMAKER_MODEL_NAME.model.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subset b. Save the artifacts to S3\n",
    "\n",
    "Finally, save the artifacts in tar.gz format to S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "os.environ['SAGEMAKER_ARTIFACTS'] = \"s3://\" + s3_bucket + \"/models/\" + model_name + \".model.tar.gz\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verfiy your work::\n",
    "\n",
    "Run the code below to print the tar.gz file from your S3 bucket. If the desired file format is not present in your S3 bucket, go back to the previous steps and re-run the steps to make sure everything is run correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "import boto3\n",
    "\n",
    "for bucket in s3.buckets.all():\n",
    "    if bucket.name == s3_bucket:\n",
    "        for obj in bucket.objects.filter(Delimiter='/models/'):\n",
    "            if obj.key in \"/models/\" + model_name + \".model.tar.gz\":\n",
    "                print('{0}:{1}'.format(bucket.name,obj.key))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Return to the Project Module 4\n",
    "\n",
    "Congratulations! You have now trained a Spark model in EMR and saved it to S3. You can now return to the project website to begin module 4, to deploy an endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark3)",
   "language": "",
   "name": "pyspark3kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark3",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
